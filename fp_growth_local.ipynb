{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'http://host-192-168-2-247-ldsa:4041'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load context\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "hostname = os.uname()[1]\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# BEN'S MASTER: 192.168.2.87\n",
    "# OUR MASTER:   192.168.2.203\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.87:7077\") \\\n",
    "        .appName(f\"load_local_comments; hostname: {hostname}\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.blockManager.port\", \"10025\")\\\n",
    "        .config(\"spark.driver.blockManager.port\", \"10026\")\\\n",
    "        .config(\"spark.driver.port\", \"10027\")\\\n",
    "        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",4)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "df_pd = pd.read_json(\"sample_reddit_comments.json\", lines=True)\n",
    "df = spark_session.createDataFrame(df_pd).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- is_submitter: boolean (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      "\n",
      "+------------------+--------------------+----------------+-----------+-------------+------+-------+------------+---------+----------+------------+-----+--------+------------------+------------+\n",
      "|            author|                body|controversiality|created_utc|distinguished|edited|     id|is_submitter|  link_id| parent_id|retrieved_on|score|stickied|         subreddit|subreddit_id|\n",
      "+------------------+--------------------+----------------+-----------+-------------+------+-------+------------+---------+----------+------------+-----+--------+------------------+------------+\n",
      "|          Dethcola|            A quarry|               0| 1506816000|         null|     0|dnqik14|       false|t3_73ieyz| t3_73ieyz|  1509189606|    3|   false|          sandiego|    t5_2qq2q|\n",
      "|        PennyBotV2|[Salutations! I'm...|               0| 1506816000|         null|     0|dnqik15|       false|t3_73g740|t1_dnqiiv7|  1509189606|    3|   false|              RWBY|    t5_2vhg0|\n",
      "|       Sir_Firebum|I got into baseba...|               0| 1506816000|         null|     0|dnqik16|       false|t3_73hlwn|t1_dnqc3lu|  1509189606|    2|   false|          baseball|    t5_2qm7u|\n",
      "|         deanzynut|        FUCKING TORY|               0| 1506816000|         null|     0|dnqik17|        true|t3_73gw9b|t1_dnqdo99|  1509189606|   18|   false|         2007scape|    t5_2wbww|\n",
      "|   OfullOstomacheO|I see a water dra...|               0| 1506816000|         null|     0|dnqik18|       false|t3_73i6z3| t3_73i6z3|  1509189606|    1|   false| mildlyinteresting|    t5_2ti4h|\n",
      "|           PlusOn3|Wait. The Michiga...|               0| 1506816000|         null|     0|dnqik19|       false|t3_73g65l|t1_dnq4z9q|  1509189606|    1|   false|            Cubers|    t5_2r6a3|\n",
      "|          yeee_bot|              ye fam|               0| 1506816000|         null|     0|dnqik1a|       false|t3_73hvr0|t1_dnqijyp|  1509189606|    2|   false|         teenagers|    t5_2rjli|\n",
      "|         grrrrreat|143417804| &gt; U...|               0| 1506816000|         null|     0|dnqik1b|        true|t3_73dvyh| t3_73dvyh|  1509189606|    1|   false|       4chan4trump|    t5_3hds7|\n",
      "|         psych4191|That is some chic...|               0| 1506816000|         null|     0|dnqik1c|       false|t3_73hgz4| t3_73hgz4|  1509189606|    2|   false|               CFB|    t5_2qm9d|\n",
      "|       fishboy2000|Does he even know...|               0| 1506816000|         null|     0|dnqik1d|       false|t3_73feje|t1_dnqick7|  1509189606|    1|   false|        rugbyunion|    t5_2qkbe|\n",
      "|  raspberryseltzer|            Tequila.|               0| 1506816000|         null|     0|dnqik1e|       false|t3_73hgz4|t1_dnqihjg|  1509189606|    2|   false|               CFB|    t5_2qm9d|\n",
      "|           Atherix|your heart beats ...|               0| 1506816000|         null|     0|dnqik1g|       false|t3_72gjlv|t1_dnmsg5o|  1509189606|    1|   false|         EchoArena|    t5_3lww9|\n",
      "|            Selash|&gt; Subscribe: /...|               0| 1506816000|         null|     0|dnqik1h|       false|t3_6vtt29|t1_dm35vvb|  1509189606|    1|   false|               HFY|    t5_2y95n|\n",
      "|    GonadusTwistus|you're really ign...|               0| 1506816000|         null|     0|dnqik1i|       false|t3_73gqbr|t1_dnqi73k|  1509189606|    2|   false|        The_Donald|    t5_38unr|\n",
      "|ithinkisaidtoomuch|lets see how deep...|               0| 1506816000|         null|     0|dnqik1j|       false|t3_73bnqg|t1_dnqhxny|  1509189606|    1|   false|        CrazyIdeas|    t5_2snxj|\n",
      "|           Brgisme|You are arguing t...|               0| 1506816000|         null|     0|dnqik1k|       false|t3_73gee6|t1_dnqhyuw|  1509189606|    2|   false|             NBA2k|    t5_2s84e|\n",
      "|      phasedarrray|I'm thinking abou...|               0| 1506816000|         null|     0|dnqik1l|       false|t3_73ha2n| t3_73ha2n|  1509189606|    2|   false|           opiates|    t5_2r0y3|\n",
      "|   ImagesOfNetwork|[Original post](h...|               0| 1506816000|         null|     0|dnqik1m|        true|t3_73ig7g| t3_73ig7g|  1509189606|    1|   false|ImagesOfNewZealand|    t5_3b6dt|\n",
      "|          calxllum|I think that's a ...|               0| 1506816000|         null|     0|dnqik1n|       false|t3_73e1zm|t1_dnqf91t|  1509189606|    3|   false|          totalwar|    t5_2rq9c|\n",
      "|           Aner123|Harp absolutelly....|               0| 1506816000|         null|     0|dnqik1o|       false|t3_64j09n| t3_64j09n|  1509189606|    1|   false|      tvcrossovers|    t5_3fkr8|\n",
      "+------------------+--------------------+----------------+-----------+-------------+------+-------+------------+---------+----------+------------+-----+--------+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fp = df.drop(*['permalink', 'gilded', 'author_flair_css_class', 'can_gild', 'author_flair_text', 'author_cakeday'])\n",
    "df_fp.printSchema()\n",
    "df_fp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Filter functions\n",
    "to_basket_unique = lambda comment: list(set((re.sub(r'\\W+', ' ', comment).lower().strip().split(' '))))\n",
    "udf_to_basket_unique = F.udf(to_basket_unique, ArrayType(StringType()))\n",
    "\n",
    "\n",
    "def filter_words(basket):\n",
    "    words_to_remove = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    return [word for word in basket if word not in words_to_remove]\n",
    "    \n",
    "udf_filter_words = F.udf(filter_words, ArrayType(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(body=['quarry']),\n",
       " Row(body=['salutations', 'imgur', 'sure', 'com', 'http', '9ttainh', 'said']),\n",
       " Row(body=['majors', 'crazy', 'got', 'cain', 'baseball', 'playing', 'teared', 'matt', 'go', 'bit', 'started', 'see', 'time']),\n",
       " Row(body=['tory', 'fucking']),\n",
       " Row(body=['see', 'water', 'dragon']),\n",
       " Row(body=['michigan', 'wait', 'club', 'like', 'state', 'u']),\n",
       " Row(body=['ye', 'fam']),\n",
       " Row(body=['2004', 'united', '2000', 'hillary', '143417804', 'buchanan', 'states', 'oldfag', '2012', 'dole', '1996', 'liakfevh', '2016', 'op', 'bush', '1984', 'obama', 'gt', '1992', 'reagan', 'kerry', 'id', '143412250', 'anonymous', '2008', '1988']),\n",
       " Row(body=['chicken', 'salad', 'running', 'shit', 'outta']),\n",
       " Row(body=['even', 'know', 'rules'])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fp1 = df_fp.withColumn('body', udf_filter_words(udf_to_basket_unique('body'))).select('body')\n",
    "df_fp1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpGrowth = FPGrowth(itemsCol=\"body\", minSupport=0.01, minConfidence=0.1)\n",
    "model = fpGrowth.fit(df_fp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|               items|freq|\n",
      "+--------------------+----+\n",
      "|            [action]| 124|\n",
      "|           [without]| 165|\n",
      "|            [making]| 105|\n",
      "|            [person]| 112|\n",
      "|           [thought]| 152|\n",
      "|              [ever]| 140|\n",
      "|            [enough]| 156|\n",
      "|             [might]| 145|\n",
      "|               [run]| 108|\n",
      "|               [bot]| 170|\n",
      "|          [bot, com]| 102|\n",
      "|       [bot, please]| 104|\n",
      "|    [bot, please, r]| 102|\n",
      "|            [bot, r]| 137|\n",
      "|     [automatically]| 117|\n",
      "|[automatically, q...| 106|\n",
      "|[automatically, q...| 106|\n",
      "|[automatically, q...| 106|\n",
      "|[automatically, q...| 106|\n",
      "|[automatically, bot]| 108|\n",
      "+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+------------------+------------------+\n",
      "|          antecedent|          consequent|        confidence|              lift|\n",
      "+--------------------+--------------------+------------------+------------------+\n",
      "|[737yli, iama, co...|[i_am_brianna_gou...|               1.0| 67.11409395973155|\n",
      "|[i_am_brianna_gou...|               [com]|               1.0| 17.57469244288225|\n",
      "|[i_am_brianna_gou...|               [www]|               1.0|23.364485981308412|\n",
      "|[i_am_brianna_gou...|             [https]|               1.0| 18.51851851851852|\n",
      "|[i_am_brianna_gou...|                 [r]|               1.0|21.978021978021978|\n",
      "|[i_am_brianna_gou...|              [iama]|               1.0| 67.11409395973155|\n",
      "|[i_am_brianna_gou...|            [737yli]|               1.0| 67.11409395973155|\n",
      "|[737yli, i_am_bri...|               [com]|               1.0| 17.57469244288225|\n",
      "|[737yli, i_am_bri...|             [https]|               1.0| 18.51851851851852|\n",
      "|[737yli, i_am_bri...|              [iama]|               1.0| 67.11409395973155|\n",
      "|              [good]|              [like]|0.2261904761904762|2.2240951444491266|\n",
      "|[737yli, i_am_bri...|             [https]|               1.0| 18.51851851851852|\n",
      "|[737yli, i_am_bri...|                 [r]|               1.0|21.978021978021978|\n",
      "|[737yli, i_am_bri...|            [reddit]|               1.0|27.247956403269754|\n",
      "|[737yli, i_am_bri...|              [iama]|               1.0| 67.11409395973155|\n",
      "|[737yli, comments...|[i_am_brianna_gou...|               1.0| 67.11409395973155|\n",
      "|[737yli, comments...|               [com]|               1.0| 17.57469244288225|\n",
      "|[737yli, comments...|             [https]|               1.0| 18.51851851851852|\n",
      "|[737yli, comments...|                 [r]|               1.0|21.978021978021978|\n",
      "|[737yli, comments...|              [iama]|               1.0| 67.11409395973155|\n",
      "+--------------------+--------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                body|          prediction|\n",
      "+--------------------+--------------------+\n",
      "|            [quarry]|                  []|\n",
      "|[salutations, img...|[bot, www, iama, ...|\n",
      "|[majors, crazy, g...|[lets, hole, deep...|\n",
      "|     [tory, fucking]|                  []|\n",
      "|[see, water, dragon]|[lets, hole, deep...|\n",
      "|[michigan, wait, ...|[way, would, make...|\n",
      "|           [ye, fam]|                  []|\n",
      "|[2004, united, 20...|                  []|\n",
      "|[chicken, salad, ...|                  []|\n",
      "| [even, know, rules]|              [like]|\n",
      "|           [tequila]|                  []|\n",
      "|[breath, provide,...|[way, make, also,...|\n",
      "|[gt, subscribe, c...|     [id, anonymous]|\n",
      "|[really, grotesqu...|[way, would, make...|\n",
      "|[deep, goes, lets...|                  []|\n",
      "|[want, hard, star...|                  []|\n",
      "|[godspeed, ending...|                  []|\n",
      "|[stuff, r, commen...|[iama, i_am_brian...|\n",
      "|[better, unit, un...|[would, also, peo...|\n",
      "|[glory, blood, fi...|                  []|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display frequent itemsets.\n",
    "model.freqItemsets.show()\n",
    "\n",
    "# Display generated association rules.\n",
    "model.associationRules.show()\n",
    "\n",
    "# transform examines the input items against all the association rules and summarize the\n",
    "# consequents as prediction\n",
    "model.transform(df_fp1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark_session.createDataFrame([\n",
    "    (0, [1, 2, 5]),\n",
    "    (1, [1, 2, 3, 5]),\n",
    "    (2, [1, 2])\n",
    "], [\"id\", \"items\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|    items|freq|\n",
      "+---------+----+\n",
      "|      [1]|   3|\n",
      "|      [2]|   3|\n",
      "|   [2, 1]|   3|\n",
      "|      [5]|   2|\n",
      "|   [5, 2]|   2|\n",
      "|[5, 2, 1]|   2|\n",
      "|   [5, 1]|   2|\n",
      "+---------+----+\n",
      "\n",
      "+----------+----------+------------------+----+\n",
      "|antecedent|consequent|        confidence|lift|\n",
      "+----------+----------+------------------+----+\n",
      "|    [5, 2]|       [1]|               1.0| 1.0|\n",
      "|       [2]|       [1]|               1.0| 1.0|\n",
      "|       [2]|       [5]|0.6666666666666666| 1.0|\n",
      "|    [2, 1]|       [5]|0.6666666666666666| 1.0|\n",
      "|       [5]|       [2]|               1.0| 1.0|\n",
      "|       [5]|       [1]|               1.0| 1.0|\n",
      "|    [5, 1]|       [2]|               1.0| 1.0|\n",
      "|       [1]|       [2]|               1.0| 1.0|\n",
      "|       [1]|       [5]|0.6666666666666666| 1.0|\n",
      "+----------+----------+------------------+----+\n",
      "\n",
      "+---+------------+----------+\n",
      "| id|       items|prediction|\n",
      "+---+------------+----------+\n",
      "|  0|   [1, 2, 5]|        []|\n",
      "|  1|[1, 2, 3, 5]|        []|\n",
      "|  2|      [1, 2]|       [5]|\n",
      "+---+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.5, minConfidence=0.6)\n",
    "model2 = fpGrowth.fit(df_test)\n",
    "\n",
    "# Display frequent itemsets.\n",
    "model2.freqItemsets.show()\n",
    "\n",
    "# Display generated association rules.\n",
    "model2.associationRules.show()\n",
    "\n",
    "# transform examines the input items against all the association rules and summarize the\n",
    "# consequents as prediction\n",
    "model2.transform(df_test).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
