{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'http://host-192-168-2-247-ldsa:4040'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load context\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "hostname = os.uname()[1]\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# BEN'S MASTER: 192.168.2.87\n",
    "# OUR MASTER:   192.168.2.203\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.87:7077\") \\\n",
    "        .appName(f\"load_local_comments; hostname: {hostname}\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.blockManager.port\", \"10025\")\\\n",
    "        .config(\"spark.driver.blockManager.port\", \"10026\")\\\n",
    "        .config(\"spark.driver.port\", \"10027\")\\\n",
    "        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",4)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "df_pd = pd.read_json(\"sample_reddit_comments.json\", lines=True)\n",
    "df = spark_session.createDataFrame(df_pd).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- is_submitter: boolean (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      "\n",
      "+------------------+--------------------+----------------+-----------+-------------+------+-------+------------+---------+----------+------------+-----+--------+------------------+------------+\n",
      "|            author|                body|controversiality|created_utc|distinguished|edited|     id|is_submitter|  link_id| parent_id|retrieved_on|score|stickied|         subreddit|subreddit_id|\n",
      "+------------------+--------------------+----------------+-----------+-------------+------+-------+------------+---------+----------+------------+-----+--------+------------------+------------+\n",
      "|          Dethcola|            A quarry|               0| 1506816000|         null|     0|dnqik14|       false|t3_73ieyz| t3_73ieyz|  1509189606|    3|   false|          sandiego|    t5_2qq2q|\n",
      "|        PennyBotV2|[Salutations! I'm...|               0| 1506816000|         null|     0|dnqik15|       false|t3_73g740|t1_dnqiiv7|  1509189606|    3|   false|              RWBY|    t5_2vhg0|\n",
      "|       Sir_Firebum|I got into baseba...|               0| 1506816000|         null|     0|dnqik16|       false|t3_73hlwn|t1_dnqc3lu|  1509189606|    2|   false|          baseball|    t5_2qm7u|\n",
      "|         deanzynut|        FUCKING TORY|               0| 1506816000|         null|     0|dnqik17|        true|t3_73gw9b|t1_dnqdo99|  1509189606|   18|   false|         2007scape|    t5_2wbww|\n",
      "|   OfullOstomacheO|I see a water dra...|               0| 1506816000|         null|     0|dnqik18|       false|t3_73i6z3| t3_73i6z3|  1509189606|    1|   false| mildlyinteresting|    t5_2ti4h|\n",
      "|           PlusOn3|Wait. The Michiga...|               0| 1506816000|         null|     0|dnqik19|       false|t3_73g65l|t1_dnq4z9q|  1509189606|    1|   false|            Cubers|    t5_2r6a3|\n",
      "|          yeee_bot|              ye fam|               0| 1506816000|         null|     0|dnqik1a|       false|t3_73hvr0|t1_dnqijyp|  1509189606|    2|   false|         teenagers|    t5_2rjli|\n",
      "|         grrrrreat|143417804| &gt; U...|               0| 1506816000|         null|     0|dnqik1b|        true|t3_73dvyh| t3_73dvyh|  1509189606|    1|   false|       4chan4trump|    t5_3hds7|\n",
      "|         psych4191|That is some chic...|               0| 1506816000|         null|     0|dnqik1c|       false|t3_73hgz4| t3_73hgz4|  1509189606|    2|   false|               CFB|    t5_2qm9d|\n",
      "|       fishboy2000|Does he even know...|               0| 1506816000|         null|     0|dnqik1d|       false|t3_73feje|t1_dnqick7|  1509189606|    1|   false|        rugbyunion|    t5_2qkbe|\n",
      "|  raspberryseltzer|            Tequila.|               0| 1506816000|         null|     0|dnqik1e|       false|t3_73hgz4|t1_dnqihjg|  1509189606|    2|   false|               CFB|    t5_2qm9d|\n",
      "|           Atherix|your heart beats ...|               0| 1506816000|         null|     0|dnqik1g|       false|t3_72gjlv|t1_dnmsg5o|  1509189606|    1|   false|         EchoArena|    t5_3lww9|\n",
      "|            Selash|&gt; Subscribe: /...|               0| 1506816000|         null|     0|dnqik1h|       false|t3_6vtt29|t1_dm35vvb|  1509189606|    1|   false|               HFY|    t5_2y95n|\n",
      "|    GonadusTwistus|you're really ign...|               0| 1506816000|         null|     0|dnqik1i|       false|t3_73gqbr|t1_dnqi73k|  1509189606|    2|   false|        The_Donald|    t5_38unr|\n",
      "|ithinkisaidtoomuch|lets see how deep...|               0| 1506816000|         null|     0|dnqik1j|       false|t3_73bnqg|t1_dnqhxny|  1509189606|    1|   false|        CrazyIdeas|    t5_2snxj|\n",
      "|           Brgisme|You are arguing t...|               0| 1506816000|         null|     0|dnqik1k|       false|t3_73gee6|t1_dnqhyuw|  1509189606|    2|   false|             NBA2k|    t5_2s84e|\n",
      "|      phasedarrray|I'm thinking abou...|               0| 1506816000|         null|     0|dnqik1l|       false|t3_73ha2n| t3_73ha2n|  1509189606|    2|   false|           opiates|    t5_2r0y3|\n",
      "|   ImagesOfNetwork|[Original post](h...|               0| 1506816000|         null|     0|dnqik1m|        true|t3_73ig7g| t3_73ig7g|  1509189606|    1|   false|ImagesOfNewZealand|    t5_3b6dt|\n",
      "|          calxllum|I think that's a ...|               0| 1506816000|         null|     0|dnqik1n|       false|t3_73e1zm|t1_dnqf91t|  1509189606|    3|   false|          totalwar|    t5_2rq9c|\n",
      "|           Aner123|Harp absolutelly....|               0| 1506816000|         null|     0|dnqik1o|       false|t3_64j09n| t3_64j09n|  1509189606|    1|   false|      tvcrossovers|    t5_3fkr8|\n",
      "+------------------+--------------------+----------------+-----------+-------------+------+-------+------------+---------+----------+------------+-----+--------+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fp = df.drop(*['permalink', 'gilded', 'author_flair_css_class', 'can_gild', 'author_flair_text', 'author_cakeday'])\n",
    "df_fp.printSchema()\n",
    "df_fp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter functions\n",
    "to_basket_unique = lambda comment: list(set((re.sub(r'\\W+', ' ', comment).lower().strip().split(' '))))\n",
    "udf_to_basket_unique = F.udf(to_basket_unique, ArrayType(StringType()))\n",
    "\n",
    "\n",
    "def filter_words(basket):\n",
    "    stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    stopwords += ['https', 'www', 'one', 'would', 'come', 'really', 'also', 'com', 'gt', 'r', '737yli']\n",
    "    stopwords += ['get', 'even', 'make', 'go', 'still', 'could', 'got', 'goes', '2', 'first', 'going', 'right', 'sure', 'something']\n",
    "    return [word for word in basket if word not in stopwords]\n",
    "    \n",
    "udf_filter_words = F.udf(filter_words, ArrayType(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(body=['quarry']),\n",
       " Row(body=['salutations', 'imgur', 'http', '9ttainh', 'said']),\n",
       " Row(body=['majors', 'crazy', 'cain', 'baseball', 'playing', 'teared', 'matt', 'bit', 'started', 'see', 'time']),\n",
       " Row(body=['tory', 'fucking']),\n",
       " Row(body=['see', 'water', 'dragon']),\n",
       " Row(body=['michigan', 'wait', 'club', 'like', 'state', 'u']),\n",
       " Row(body=['ye', 'fam']),\n",
       " Row(body=['2004', 'united', '2000', 'hillary', '143417804', 'buchanan', 'states', 'oldfag', '2012', 'dole', '1996', 'liakfevh', '2016', 'op', 'bush', '1984', 'obama', '1992', 'reagan', 'kerry', 'id', '143412250', 'anonymous', '2008', '1988']),\n",
       " Row(body=['chicken', 'salad', 'running', 'shit', 'outta']),\n",
       " Row(body=['know', 'rules'])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fp1 = df_fp.withColumn('body', udf_filter_words(udf_to_basket_unique('body'))).select('body')\n",
    "df_fp1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpGrowth = FPGrowth(itemsCol=\"body\", minSupport=0.01, minConfidence=0.05)\n",
    "model = fpGrowth.fit(df_fp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from functools import reduce\n",
    "extract_top = [el for lis in map(lambda x: x.items, model.freqItemsets.sort(\"freq\", ascending = False).select('items').take(30)) for el in lis]\n",
    "#extract_top = reduce(list.__add__, map(lambda x: x.items, model.freqItemsets.sort(\"freq\", ascending = False).select('items').take(30)))\n",
    "print(extract_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'see', 'people', 'deleted', 'think', 'good', 'time', 'know', 'reddit', 'way', 'much', 'well', 'want', 'game', 'deep', 'removed', 'lets', 'hole', 'comments', 'need', 'deep', 'see', 'use', 'hole', 'see', 'lets', 'see', 'though', 'rabbit', 'rabbit', 'lets', 'rabbit', 'lets', 'deep', 'rabbit', 'lets', 'deep', 'see', 'rabbit', 'lets', 'see']\n",
      "+----------+----+\n",
      "|     items|freq|\n",
      "+----------+----+\n",
      "|    [like]|1017|\n",
      "|     [see]| 639|\n",
      "|  [people]| 553|\n",
      "| [deleted]| 529|\n",
      "|   [think]| 510|\n",
      "|    [good]| 504|\n",
      "|    [time]| 469|\n",
      "|    [know]| 424|\n",
      "|  [reddit]| 367|\n",
      "|     [way]| 367|\n",
      "|    [much]| 346|\n",
      "|    [well]| 336|\n",
      "|    [want]| 327|\n",
      "|    [game]| 288|\n",
      "|    [deep]| 282|\n",
      "| [removed]| 278|\n",
      "|    [lets]| 273|\n",
      "|    [hole]| 271|\n",
      "|[comments]| 270|\n",
      "|    [need]| 268|\n",
      "+----------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+----------+----------+----+\n",
      "|antecedent|consequent|confidence|lift|\n",
      "+----------+----------+----------+----+\n",
      "+----------+----------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display frequent itemsets.\n",
    "model.freqItemsets.sort(\"freq\", ascending = False).show()\n",
    "\n",
    "# Display generated association rules.\n",
    "model.associationRules.filter(F.array_contains('antecedent', 'removed')).show()\n",
    "\n",
    "# transform examines the input items against all the association rules and summarize the\n",
    "# consequents as prediction\n",
    "#model.transform(df_fp1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
